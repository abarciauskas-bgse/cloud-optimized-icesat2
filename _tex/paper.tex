% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{agujournal2019}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{url} %this package should fix any errors with URLs in refs.
\usepackage{lineno}
\usepackage[inline]{trackchanges} %for better track changes. finalnew option will compile document with changes incorporated.
\usepackage{soul}
\linenumbers
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Evaluating Cloud-Optimized HDF5 for NASA's ICESat-2 Mission},
  pdfauthor={Luis A. Lopez; Andrew P. Barrett; Amy Steiker; Aleksandar Jelenak; Lisa Kaser; Jeffrey E. Lee},
  pdfkeywords={cloud-native, cloud, HDF5, NASA, ICESat-2},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\journalname{Geophysical Research Letters}

\draftfalse

\begin{document}
\title{Evaluating Cloud-Optimized HDF5 for NASA's ICESat-2 Mission}

\authors{Luis A. Lopez\affil{1}, Andrew P. Barrett\affil{1}, Amy
Steiker\affil{1}, Aleksandar Jelenak\affil{2}, Lisa
Kaser\affil{1}, Jeffrey E. Lee\affil{3}}
\affiliation{1}{CIRES, National Snow and Ice Data Center, University of
Colorado, Boulder., Boulder, CO, USA}\affiliation{2}{The HDF
Group, Champaign, IL, USA}\affiliation{3}{NASA Goddard Space Flight
Center, NASA / KBR, Greenbelt, MD, USA}



\begin{abstract}
The Hierarchical Data Format (HDF) is a common archival format for
n-dimensional scientific data; it has been utilized to store valuable
information from astrophysics to earth sciences and everything in
between. As flexible and powerful as HDF can be, it comes with big
tradeoffs when it's accessed from remote storage systems, mainly because
the file format and the client I/O libraries were designed for local and
supercomputing workflows. As scientific data and workflows migrate to
the cloud , efficient access to data stored in HDF format is a key
factor that will accelerate or slow down ``science in the cloud'' across
all disciplines. We present an implementation of recently available
features in the HDF5 stack that results in performant access to HDF from
remote cloud storage. This performance is on par with modern
cloud-native formats like Zarr but with the advantage of not having to
reformat data or generate metadata sidecar files (DMR++, Kerchunk). Our
benchmarks also show potential cost-savings for data producers if their
data are processed using cloud-optimized strategies.
\end{abstract}





\section{Problem}\label{problem}

Scientific data from NASA and other agencies are increasingly being
distributed from the commercial cloud. Cloud storage enables large-scale
workflows and should reduce local storage costs. It also allows the use
of scalable on-demand cloud computing resources by individual scientists
and the broader scientific community. However, the majority of this
scientific data is stored in a format that was not designed for the
cloud: The Hierarchical Data format or HDF.

The most recent version of the Hierarchical Data Format is HDF5, a
common archival format for n-dimensional scientific data; it has been
utilized to store valuable information from astrophysics to earth
sciences and everything in between. As flexible and powerful as HDF5 can
be, it comes with big tradeoffs when it's accessed from remote storage
systems.

HDF5 is a complex file format; we can think of it as a file system using
a tree-like structure with multiple data types and native data
structures. Because of this complexity, the most reliable way of
accessing data stored in this format is using the HDF5 C API. Regardless
of access pattern, nearly all tools ultimately rely on the HDF5-C
library and this brings a couple issues that affect the efficiency of
accessing this format over the network:

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{\texorpdfstring{\textbf{Metadata
fragmentation}}{Metadata fragmentation}}\label{metadata-fragmentation}

By default, file-level metadata associated with a dataset is stored in
chunks of 4kb. This produces a lot of fragmentation across the file
especially for data with many variables and nested groups.

\subsubsection{\texorpdfstring{\textbf{Global API
Lock}}{Global API Lock}}\label{global-api-lock}

Because of the historical complexity of operations with the HDF5 format,
there has been a necessity to make the library thread-safe and similarly
to what happens in the Python language, the simplest mechanism to
implement this is to have a global API lock. This global lock is not as
big of an issue when we read data from local disk but it becomes a major
bottleneck when we read data over the network because each read is
sequential and latency in the cloud is exponentially bigger than local
access.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure*}

\centering{

\includegraphics{figures/figure-1.png}

}

\caption{\label{fig-1}shows how reads (Rn) are done in order to access
file metadata, In the first read, R0, the HDF5 library verifies the file
signature from the superblock, subsequent reads, R1, R2,\ldots Rn, read
file metadata, 4kb at the time.}

\end{figure*}%

\subsubsection{Background and data
selection}\label{background-and-data-selection}

As a result of community feedback and ``hack weeks'' organized by NSIDC
and UW eScience Institute in 2023, NSIDC started the Cloud Optimized
Format Investigation (COFI) project to improve access to HDF5 from the
ICESat-2 mission. A spaceborne lidar that retrieves surface topography
of the Earth's ice sheets, land and (oceans Neumann et al., 2019).
Because of its complexity, large size and importance for cryospheric
studies we targeted the ATL03 dataset. ATL03 core data are geolocated
photon heights from the ICESat-2 ATLAS instrument. Each file contains
1003 geophysical variables in 6 data groups. Although our research was
focused on this dataset, most of our findings are applicable to any
dataset stored in HDF5 and NetCDF4.

\section{Methodology}\label{methodology}

We tested access times to original and cloud-optimized small (1 GB),
medium (2 GB) and large (7 GB) HDF5 ATL03 files {[}list files tested{]}
stored in AWS S3 buckets in region us-west-2, the region hosting NASA's
Earthdata Cloud archives. Files were accessed using Python tools
commonly used by Earth scientists: h5py and Xarray. h5py is a Python
wrapper around the HDF5 C API. xarray\footnote{\texttt{h5py} is a
  dependency of Xarray} is a widely used Python package for working with
n-dimensional data. We also tested access times using h5coro, a python
package optimized for reading HDF5 files from S3 buckets and kerchunk, a
tool that creates an efficient lookup table for file chunks to allow
performant partial reads of files.

HDF5 ATL03 files were originally cloud optimized by ``repacking'' them,
using a relatively new feature in the HDF5 C API called ``paged
aggregation''. Page aggregation does 2 things: it collects file-level
metadata from datasets and stores it on dedicated metadata blocks in the
file; and it forces the library to write data using fixed-size blocks.
Aggregation allows client libraries to read file metadata with only a
few requests and uses the page size used in the aggregation as the
minimal request size, overriding the 1 request per chunk behavior.

\begin{figure*}

\centering{

\includegraphics{figures/figure-2.png}

}

\caption{\label{fig-2}shows how file-level metadata and data gets
internally packed once we use paged aggregation on a file.}

\end{figure*}%

\section{Results}\label{results}

\begin{figure*}

\centering{

\includegraphics{figures/figure-3.png}

}

\caption{\label{fig-3}Benchmarks show that cloud optimizing ATL03 files
improved access times at least an order of magnitude when used with
aligned I/O patterns, this is telling the library about the cloud
optimization and page size.}

\end{figure*}%

\section{Recommendations}\label{recommendations}

We have split our recommendations for the ATL03 product into 3 main
categories, creating the files, accessing the files, and future tool
development.

\subsection{Recommended cloud
optimizations}\label{recommended-cloud-optimizations}

Based on our testing we recommend the following cloud optimizations for
creating HDF5 files for the ATL03 product: Create HDF5 files using paged
aggregation by setting HDF5 library parameters:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  File page strategy: H5F\_FSPACE\_STRATEGY\_PAGE
\item
  File page size: 8000000 If repacking an existing file, h5repack
  contains the code to alter tese variables inside the file
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
 \ExtensionTok{h5repack} \AttributeTok{{-}S}\NormalTok{ PAGE }\AttributeTok{{-}G}\NormalTok{ 8000000 input.h5 output.h5}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Avoid using unlimited dimensions when creating variables because the
  HDF5 API cannot support it inside bffered pages and representation of
  these variables is not supported by Kerchunk.
\end{enumerate}

\subsubsection{Reasoning}\label{reasoning}

Based on the variable size of ATL03 it becomes really difficult to
allocate a fixed metadata page, big files contain north of 30MB of
metadata, but the median sized file is below 8MB. If we had adopted user
block we would have caused an increase in the file size and storage cost
of approximate 30\% (reference to our tests). Another consequence of
using a dedicated fixed page for file-level metadata is that metadata
overflow will generate the same impact in access times, the library will
fetch the metadata in one go but the rest will be using the predefined
block size of 4kb.

Paged aggregation is thus the simplest way of cloud optimizing an HDF5
file as the metadata will keep filling dedicated pages until all the
file-level metadata is stored at the front of the file. Chunk sizes
cannot be larger than the page size and when chunk sizes are smaller we
need to take into account how these chunks will fit on a page, in an
ideal scenario all the space will be filled but that is not the case and
we will end up with unused space See~\ref{fig-2}.

\subsection{Recommended access
patterns}\label{recommended-access-patterns}

Placeholder

\subsection{Recommended tooling
development}\label{recommended-tooling-development}

Placeholder

\subsection{Mission implementation}\label{mission-implementation}

ATL03 is a complex science data product containing both segmented (20
meters along-track) and large, variable-rate photon datasets. ATL03 is
created using pipeline-style processing where the science data and
NetCDF-style metadata are written by independent software packages. The
following steps were employed to create cloud-optimized Release 007
ATL03 products, while minimizing increases in file size:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Set the ``file space strategy'' to H5F\_FSPACE\_STRATEGY\_PAGE and
  enabled ``free space tracking'' within the HDF5 file creation property
  list.
\item
  Set the ``file space page size'' to 8MiB.
\item
  Changed all ``COMPACT'' dataset storage types to ``CONTIGUOUS''.
\item
  Increased the ``chunk size'' of the photon-rate datasets (from 10,000
  to 100,000 elements), while making sure no ``chunk sizes'' exceeded
  the 8MiB ``file space page size''.
\item
  Introduced a new production step that executes the ``h5repack''
  utility (with no options) to create a ``defragmented'' final product.
\end{enumerate}

\section{Discussion}\label{discussion}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Chunking shapes and sizes
\item
  Paged aggregation vs User block
\item
  Side effects on different access patterns, e.g.~Kerchunk
\end{enumerate}

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\vspace{1em}

\bibitem[\citeproctext]{ref-NEUMANN2019111325}
Neumann, T. A., Martino, A. J., Markus, T., Bae, S., Bock, M. R.,
Brenner, A. C., et al. (2019). The ice, cloud, and land elevation
satellite -- 2 mission: A global geolocated photon product derived from
the advanced topographic laser altimeter system. \emph{Remote Sensing of
Environment}, \emph{233}, 111325.
https://doi.org/\url{https://doi.org/10.1016/j.rse.2019.111325}

\end{CSLReferences}




\end{document}
